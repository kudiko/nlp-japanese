{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83c5d390",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import neologdn\n",
    "import pandas as pd\n",
    "# neologdn.normalize(\"ﾊﾝｶｸｶﾅ\")\n",
    "\n",
    "song_id = []\n",
    "song_name = []\n",
    "year = []\n",
    "text = []\n",
    "\n",
    "\n",
    "\n",
    "import os, glob\n",
    "\n",
    "os.chdir('../parse_results/')\n",
    "for filename in glob.glob('*.txt'):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f: # open in readonly mode\n",
    "        lines = f.readlines()\n",
    "        if (len(lines) != 3):\n",
    "            continue\n",
    "        song_id.append(int(filename.replace('.txt', '')))\n",
    "        song_name.append(lines[0].rstrip())\n",
    "        year.append(int(lines[1]))\n",
    "        text.append(neologdn.normalize(lines[2]).rstrip())\n",
    "#         data = neologdn.normalize(data)\n",
    "#         print(data)\n",
    "\n",
    "song_id, song_name, year, text = (list(t) for t in zip(*sorted(zip(song_id, song_name, year, text))))\n",
    "\n",
    "# print(song_id)\n",
    "# print(song_name)\n",
    "# print(year)\n",
    "# print(text)\n",
    "\n",
    "# db = pd.DataFrame({'song_id' : song_id, 'song_name' : song_name, 'year' : year, 'text' : text})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb5e6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_transformers\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ja_core_news_trf\")\n",
    "import ja_core_news_trf\n",
    "nlp = ja_core_news_trf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985a7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['a', 'b']\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2171250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 16000 at 2024-05-05 11:52:14.088681\n",
      "processing 17000 at 2024-05-05 12:03:06.313572\n",
      "processing 18000 at 2024-05-05 12:14:07.325957\n",
      "processing 19000 at 2024-05-05 12:23:46.359712\n",
      "processing 20000 at 2024-05-05 12:33:44.380267\n",
      "processing 21000 at 2024-05-05 12:42:56.494835\n",
      "processing 22000 at 2024-05-05 12:53:22.765870\n",
      "processing 23000 at 2024-05-05 13:02:22.187177\n",
      "processing 24000 at 2024-05-05 13:12:28.607304\n",
      "processing 25000 at 2024-05-05 13:22:08.563935\n",
      "processing 26000 at 2024-05-05 13:30:59.539881\n",
      "processing 27000 at 2024-05-05 13:40:13.138052\n",
      "processing 28000 at 2024-05-05 13:49:13.035628\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(text[0])\n",
    "# # doc\n",
    "# print([(w.text, w.pos_) for w in doc])\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "allowed_pos = frozenset(['ADJ', #adjective\n",
    "                        'ADV', #adverb\n",
    "                        'INTJ', #interjection,  ex. \"psst, ouch, bravo, hello\"\n",
    "                        'NOUN',\n",
    "                        'PROPN', #proper noun, ex. 'London, Mary'\n",
    "                        'VERB'\n",
    "                        ])\n",
    "\n",
    "def filter_parts_of_speech(nlp_processed_array):\n",
    "    english_words = []\n",
    "    japanese_words = []\n",
    "    for w in nlp_processed_array:\n",
    "        # print(w.text, w.pos_)\n",
    "        cur_pos = w.pos_\n",
    "        if (cur_pos not in allowed_pos):\n",
    "            continue\n",
    "\n",
    "        if (cur_pos == 'NOUN'):\n",
    "            m = re.search('[a-zA-Z]', w.text)\n",
    "            if (m is not None):\n",
    "                english_words.append(w.text)\n",
    "                continue\n",
    "\n",
    "        japanese_words.append(w.text)\n",
    "\n",
    "\n",
    "    return japanese_words, english_words\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_single_song_2_bag_of_words(text):\n",
    "    doc = nlp(text)\n",
    "    return filter_parts_of_speech(doc)\n",
    "    # print(jap_words)\n",
    "    # print(eng_words)\n",
    "\n",
    "\n",
    "# texts_bag_of_words = []\n",
    "os.chdir('../test/')\n",
    "nlp_out_file = open(\"nlp_out_0.txt\", 'a')\n",
    "\n",
    "log_file = open(\"log.txt\", 'a')\n",
    "\n",
    "for song_ind in range(16000, len(text)):\n",
    "    if song_ind % 1000 == 0:\n",
    "        nlp_out_file.close()\n",
    "        nlp_out_file = open(\"nlp_out_\" + str(song_ind) + \".txt\", 'a')\n",
    "        print('processing ' + str(song_ind) + ' at ' + str(datetime.datetime.now()))\n",
    "        log_file.write('processing ' + str(song_ind) + ' at ' + str(datetime.datetime.now()) + '\\n')\n",
    "    jap_words, eng_words = process_single_song_2_bag_of_words(text[song_ind])\n",
    "\n",
    "    out = str(song_id[song_ind]) + '|' + str(year[song_ind]) + '|'\n",
    "\n",
    "    for word in jap_words:\n",
    "        out += word + ','\n",
    "    \n",
    "    out += '|'\n",
    "\n",
    "    for word in eng_words:\n",
    "        out += word + ','\n",
    "\n",
    "    out += '\\n'\n",
    "    # print(out)\n",
    "    nlp_out_file.write(out)\n",
    "    \n",
    "    # texts_bag_of_words.append(jap_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2e0f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da9c6b59",
   "metadata": {},
   "source": [
    "ID 273692 has only english in jap block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a0603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earlier than 1950: 2\n",
      "1950s: \n",
      "0\n",
      "1960s: \n",
      "19\n",
      "1970s: \n",
      "120\n",
      "1980s: \n",
      "2605\n",
      "1990s: \n",
      "34190\n",
      "2000s: \n",
      "88619\n",
      "2010s: \n",
      "155059\n",
      "2020s: \n",
      "66116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "346730"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('earlier than 1950: ' + str(len(db[db['year'] < 1950])))\n",
    "db.head()\n",
    "def decade_slice(df):\n",
    "    for decade in range(1950, 2030, 10):\n",
    "        print(str(decade) + 's: ')\n",
    "        print(len(db[(db['year'] >= decade) & (db['year'] < decade + 10)]))\n",
    "#         db[(db['year'] > 2010) & (db['year'] < 2025)]\n",
    "\n",
    "decade_slice(db)\n",
    "\n",
    "len(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kudiko/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/kudiko/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy_transformers\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "import en_core_web_trf\n",
    "nlp = en_core_web_trf.load()\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "# import unidic2ud.spacy\n",
    "# nlp = unidic2ud.spacy.load(\"kindai\", parser=\"ja_core_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('これ', 'PRON'), ('は', 'ADP'), ('文章', 'NOUN'), ('です', 'AUX'), ('。', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ja_core_news_trf\")\n",
    "import ja_core_news_trf\n",
    "nlp = ja_core_news_trf.load()\n",
    "doc = nlp(\"これは文章です。\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c57da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
